=-=-=-= date and time of query: 2023-02-15 22:46:51.724801 =-=-=-=
user input: what is the weather like today on the College of charleston Campus?


> Entering new AgentExecutor chain...
 I need to find out the weather for the College of Charleston Campus
Action: Open Meteo API
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.8/site-packages/gradio/routes.py", line 374, in run_predict
    output = await app.get_blocks().process_api(
  File "/home/user/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1017, in process_api
    result = await self.call_function(
  File "/home/user/.local/lib/python3.8/site-packages/gradio/blocks.py", line 835, in call_function
    prediction = await anyio.to_thread.run_sync(
  File "/home/user/.local/lib/python3.8/site-packages/anyio/to_thread.py", line 31, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/home/user/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 937, in run_sync_in_worker_thread
    return await future
  File "/home/user/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 867, in run
    result = context.run(func, *args)
  File "app.py", line 39, in chat
    agent_response = agent.run(user_input)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 239, in run
    return self(args[0])[self.output_keys[0]]
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 142, in __call__
    raise e
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 139, in __call__
    outputs = self._call(inputs)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/agents/agent.py", line 554, in _call
    next_step_output = self._take_next_step(
  File "/home/user/.local/lib/python3.8/site-packages/langchain/agents/agent.py", line 426, in _take_next_step
    raise e
  File "/home/user/.local/lib/python3.8/site-packages/langchain/agents/agent.py", line 421, in _take_next_step
    observation = tool.func(output.tool_input)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 239, in run
    return self(args[0])[self.output_keys[0]]
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 142, in __call__
    raise e
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 139, in __call__
    outputs = self._call(inputs)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/api/base.py", line 76, in _call
    answer = self.api_answer_chain.predict(
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/llm.py", line 154, in predict
    return self(kwargs)[self.output_key]
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 142, in __call__
    raise e
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/base.py", line 139, in __call__
    outputs = self._call(inputs)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/llm.py", line 135, in _call
    return self.apply([inputs])[0]
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/llm.py", line 117, in apply
    response = self.generate(input_list)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/chains/llm.py", line 59, in generate
    response = self.llm.generate(prompts, stop=stop)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/llms/base.py", line 128, in generate
    raise e
  File "/home/user/.local/lib/python3.8/site-packages/langchain/llms/base.py", line 125, in generate
    output = self._generate(prompts, stop=stop)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/llms/openai.py", line 259, in _generate
    response = self.completion_with_retry(prompt=_prompts, **params)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/llms/openai.py", line 206, in completion_with_retry
    return _completion_with_retry(**kwargs)
  File "/home/user/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
  File "/home/user/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "/home/user/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/user/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "/home/user/.local/lib/python3.8/site-packages/langchain/llms/openai.py", line 204, in _completion_with_retry
    return self.client.create(**kwargs)
  File "/home/user/.local/lib/python3.8/site-packages/openai/api_resources/completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/home/user/.local/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/home/user/.local/lib/python3.8/site-packages/openai/api_requestor.py", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/home/user/.local/lib/python3.8/site-packages/openai/api_requestor.py", line 619, in _interpret_response
    self._interpret_response_line(
  File "/home/user/.local/lib/python3.8/site-packages/openai/api_requestor.py", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 6681 tokens (6425 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.