{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and api_keys\n",
    "from langchain import OpenAI, LLMMathChain, SerpAPIWrapper, WolframAlphaAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# read in the api key from file apikeys.txt. with file path C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\data-534_expo_proj\\openai_key.txt\n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\openai_key.txt\", \"r\") as f:\n",
    "    openai_api_key = f.read()\n",
    "\n",
    "# read in serp api key from C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\data-534_expo_proj\\serpapi_key.txt\n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\serpapi_key.txt\", \"r\") as f:\n",
    "    serp_api_key = f.read()\n",
    "\n",
    "# read in wolfram alpha api key from C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\data-534_expo_proj\\wolfram_appid.txt\n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\wolfram_appid.txt\", \"r\") as f:\n",
    "    wolfram_appid = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "llm1 = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "search = SerpAPIWrapper(serpapi_api_key=serp_api_key)\n",
    "llm_math_chain = LLMMathChain(llm=llm1, verbose=True)\n",
    "wolfram = WolframAlphaAPIWrapper(wolfram_alpha_appid=wolfram_appid)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer simple questions about math\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if model is working\n",
    "# mrkl.run(\"\"\"Solve 312*s + 276*s - 661*s + 952 = -362 for s.\"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the test from C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\data-534_expo_proj\\Synergistic_Computing_hf\\benchmarking\\deepmind_math_q.txt\n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\deepmind_math_q.txt\", \"r\") as f:\n",
    "    test = f.read()\n",
    "\n",
    "# the test data is formated as follows: \n",
    "# Which is the nearest to 6?  (a) -462/107  (b) 0.3  (c) 6/19\n",
    "# c\n",
    "# as you can see the first line holds the question and the second line holds the answer.\n",
    "# please create a data structure that holds the questions and answers in a list of tuples\n",
    "# for example: [(\"Which is the nearest to 6?  (a) -462/107  (b) 0.3  (c) 6/19\", \"c\"), ...]\n",
    "\n",
    "# split the test data into a list of questions and answers\n",
    "test = test.split(\"\\n\")\n",
    "\n",
    "# create a list of tuples that hold the questions and answers\n",
    "test = [(test[i], test[i+1]) for i in range(0, len(test), 2)]\n",
    "\n",
    "# this is for testing purposes only. please comment out this line when you are done testing\n",
    "# # cut down the test data to 2 questions\n",
    "# test = test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing my model with a set of math questions from deepmind (https://github.com/deepmind/mathematics_dataset)\n",
    "# redirect stdout to file for logging\n",
    "import sys\n",
    "import re\n",
    "sys.stdout = open('C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\MRKL_chat_test.txt', 'w')\n",
    "i = 1\n",
    "# create a variable to hold the number of questions\n",
    "num_questions = len(test)\n",
    "# create a variable to hold the number of correct answers\n",
    "num_correct = 0\n",
    "# loop through the questions and ask mrkl each question\n",
    "for question, answer in test:\n",
    "    print(f'=============================Q{i}==============================================================\\n')\n",
    "    # with nice formating write the question that the agent will be asked\n",
    "    print(f\"Question: {question} \\n\")\n",
    "    # ask the agent the question\n",
    "    print('---------------------------------------------------------------\\n')\n",
    "    # try to have the agent answer the question. if the agent fails to answer the question then return agent failed to answer and move on to the next question\n",
    "    try:\n",
    "        result = mrkl.run(question)\n",
    "    except Exception as e:\n",
    "        print(f\"Agent's Answer: Agent Failed to Answer \\n\")\n",
    "        print(f'==============================================================================================\\n')\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    print('---------------------------------------------------------------\\n')\n",
    "    # with nice formating write the result that the agent gave\n",
    "    print(f\"Agent's Answer: {result} \\n\")\n",
    "    print('---------------------------------------------------------------\\n')\n",
    "    # with nice formating write the answer that the agent will be asked\n",
    "    print(f\"Actual Answer: {answer} \\n\")\n",
    "    # evaluate if the agent got the answer correct. use regex to see if the answer is in the result\n",
    "    if re.search(answer, result):\n",
    "        # if the answer is in the result then the agent got the answer correct\n",
    "        num_correct += 1\n",
    "        print(f\"Agent's Answer: Correct \\n\")\n",
    "    print(f'==============================================================================================\\n')\n",
    "    i += 1\n",
    "\n",
    "# print the number of questions and the number of correct answers\n",
    "print(f\"Number of Questions: {num_questions} \\n\")\n",
    "print(f\"Number of Correct Answers: {num_correct} \\n\")\n",
    "print(f\"Accuracy: {num_correct/num_questions} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b) 0.3\n"
     ]
    }
   ],
   "source": [
    "# set up normal openai davinci model to compare to\n",
    "import openai\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def query_davinci_old_version(prompt, model_version=\"text-davinci-003\"):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_version,\n",
    "        prompt=prompt,\n",
    "        max_tokens=40,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.15\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Which is the nearest to 6?  (a) -462/107  (b) 0.3  (c) 6/19\"\n",
    "response_text = query_davinci_old_version(prompt)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions: 105 \n",
      "\n",
      "Number of Correct Answers: 37 \n",
      "\n",
      "Accuracy: 0.3523809523809524 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# testing regular open ai model \n",
    "# create a new empty txt file to hold the results \n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\deepmind_math_q_results.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "j = 1\n",
    "num_correct = 0\n",
    "num_questions = len(test)\n",
    "\n",
    "# loop through the questions and ask mrkl each question\n",
    "for question, answer in test:\n",
    "    try:\n",
    "        result = query_davinci_old_version(question)\n",
    "    except Exception as e:\n",
    "        result = \"Agent Failed to Answer\"\n",
    "        with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\deepmind_math_q_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(f'=============================Q{j}==============================================================\\n')\n",
    "            # with nice formating write the question that the agent will be asked\n",
    "            f.write(f\"Question: {question} \\n\")\n",
    "            f.write('---------------------------------------------------------------\\n')\n",
    "            # with nice formating write the result that the agent gave\n",
    "            f.write(f\"Agent's Answer:\\n {result} \\n\")\n",
    "            f.write('---------------------------------------------------------------\\n')\n",
    "            # with nice formating write the answer that the agent will be asked\n",
    "            f.write(f\"Actual Answer: {answer} \\n\")\n",
    "            f.write(f'==============================================================================================\\n')\n",
    "        i += 1\n",
    "        continue\n",
    "    # write the question and answer to the results file\n",
    "    with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\deepmind_math_q_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(f'=============================Q{j}==============================================================\\n')\n",
    "        # with nice formating write the question that the agent will be asked\n",
    "        f.write(f\"Question: {question} \\n\")\n",
    "        f.write('---------------------------------------------------------------\\n')\n",
    "        # with nice formating write the result that the agent gave\n",
    "        f.write(f\"Agent's Answer:\\n {result} \\n\")\n",
    "        f.write('---------------------------------------------------------------\\n')\n",
    "        # with nice formating write the answer that the agent will be asked\n",
    "        f.write(f\"Actual Answer: {answer} \\n\")\n",
    "        if re.search(answer, result):\n",
    "            # if the answer is in the result then the agent got the answer correct\n",
    "            num_correct += 1\n",
    "            f.write(f\"Agent's Answer: Correct \\n\")\n",
    "        f.write(f'==============================================================================================\\n')\n",
    "        \n",
    "    j += 1\n",
    "\n",
    "with open(\"C:\\\\Users\\\\cayde\\\\Desktop\\\\Grad_School_stuff\\\\data-534_expo_proj\\\\Synergistic_Computing_hf\\\\benchmarking\\\\deepmind_math_q_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "    f.write(f\"Number of Questions: {num_questions} \\n\")\n",
    "    f.write(f\"Number of Correct Answers: {num_correct} \\n\")\n",
    "    f.write(f\"Accuracy: {num_correct/num_questions} \\n\")\n",
    "\n",
    "# print the number of questions and the number of correct answers\n",
    "print(f\"Number of Questions: {num_questions} \\n\")\n",
    "print(f\"Number of Correct Answers: {num_correct} \\n\")\n",
    "print(f\"Accuracy: {num_correct/num_questions} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
